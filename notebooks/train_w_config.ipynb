{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ec7ff3",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ba3a8-1af8-43f9-bc82-1150526864c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d80ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import distutils.util\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7d9a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from monai.apps.deepedit.interaction import Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31801cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monai.apps.deepedit.transforms import (\n",
    "    AddGuidanceSignalDeepEditd,\n",
    "    AddRandomGuidanceDeepEditd,\n",
    "    FindDiscrepancyRegionsDeepEditd,\n",
    "    NormalizeLabelsInDatasetd,\n",
    "    FindAllValidSlicesMissingLabelsd,\n",
    "    AddInitialSeedPointMissingLabelsd,\n",
    "    SplitPredsLabeld,\n",
    ")\n",
    "from monai.data import partition_dataset\n",
    "from monai.data.dataloader import DataLoader\n",
    "from monai.data.dataset import PersistentDataset\n",
    "from monai.engines import SupervisedEvaluator, SupervisedTrainer\n",
    "from monai.handlers import (\n",
    "    CheckpointSaver,\n",
    "    LrScheduleHandler,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    TensorBoardStatsHandler,\n",
    "    ValidationHandler,\n",
    "    from_engine,\n",
    ")\n",
    "from monai.inferers import SimpleInferer\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.networks.nets import DynUNet, UNETR\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AddChanneld,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandShiftIntensityd,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    ScaleIntensityRanged,\n",
    "    ToNumpyd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293a7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(network, labels, spatial_size):\n",
    "    # Network\n",
    "    if network == \"unetr\":\n",
    "        network = UNETR(\n",
    "            spatial_dims=3,\n",
    "            in_channels=len(labels) + 1,\n",
    "            out_channels=len(labels),\n",
    "            img_size=spatial_size,\n",
    "            feature_size=64,\n",
    "            hidden_size=1536,\n",
    "            mlp_dim=3072,\n",
    "            num_heads=48,\n",
    "            pos_embed=\"conv\",\n",
    "            norm_name=\"instance\",\n",
    "            res_block=True,\n",
    "        )\n",
    "    else:\n",
    "        network = DynUNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=len(labels) + 1,\n",
    "            out_channels=len(labels),\n",
    "            kernel_size=[3, 3, 3, 3, 3, 3],\n",
    "            strides=[1, 2, 2, 2, 2, [2, 2, 1]],\n",
    "            upsample_kernel_size=[2, 2, 2, 2, [2, 2, 1]],\n",
    "            norm_name=\"instance\",\n",
    "            deep_supervision=False,\n",
    "            res_block=True,\n",
    "        )\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5655410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_transforms(labels, spatial_size):\n",
    "    t = [\n",
    "        LoadImaged(keys=(\"image\", \"label\"), reader=\"ITKReader\"),\n",
    "        NormalizeLabelsInDatasetd(keys=\"label\", label_names=labels),\n",
    "        AddChanneld(keys=(\"image\", \"label\")),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # This transform may not work well for MR images\n",
    "        ScaleIntensityRanged(keys=\"image\", a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        RandFlipd(keys=(\"image\", \"label\"), spatial_axis=[0], prob=0.10),\n",
    "        RandFlipd(keys=(\"image\", \"label\"), spatial_axis=[1], prob=0.10),\n",
    "        RandFlipd(keys=(\"image\", \"label\"), spatial_axis=[2], prob=0.10),\n",
    "        RandRotate90d(keys=(\"image\", \"label\"), prob=0.10, max_k=3),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.10, prob=0.50),\n",
    "        Resized(keys=(\"image\", \"label\"), spatial_size=spatial_size, mode=(\"area\", \"nearest\")),\n",
    "        # Transforms for click simulation\n",
    "        FindAllValidSlicesMissingLabelsd(keys=\"label\", sids=\"sids\"),\n",
    "        AddInitialSeedPointMissingLabelsd(keys=\"label\", guidance=\"guidance\", sids=\"sids\"),\n",
    "        AddGuidanceSignalDeepEditd(keys=\"image\", guidance=\"guidance\"),\n",
    "        #\n",
    "        ToTensord(keys=(\"image\", \"label\")),\n",
    "    ]\n",
    "\n",
    "    return Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b42c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_click_transforms():\n",
    "    t = [\n",
    "        Activationsd(keys=\"pred\", softmax=True),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True),\n",
    "        ToNumpyd(keys=(\"image\", \"label\", \"pred\")),\n",
    "        # Transforms for click simulation\n",
    "        FindDiscrepancyRegionsDeepEditd(keys=\"label\", pred=\"pred\", discrepancy=\"discrepancy\"),\n",
    "        AddRandomGuidanceDeepEditd(\n",
    "            keys=\"NA\",\n",
    "            guidance=\"guidance\",\n",
    "            discrepancy=\"discrepancy\",\n",
    "            probability=\"probability\",\n",
    "        ),\n",
    "        AddGuidanceSignalDeepEditd(keys=\"image\", guidance=\"guidance\"),\n",
    "        #\n",
    "        ToTensord(keys=(\"image\", \"label\")),\n",
    "    ]\n",
    "\n",
    "    return Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22144fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_post_transforms(labels):\n",
    "    t = [\n",
    "        Activationsd(keys=\"pred\", softmax=True),\n",
    "        AsDiscreted(\n",
    "            keys=(\"pred\", \"label\"),\n",
    "            argmax=(True, False),\n",
    "            to_onehot=(len(labels), len(labels)),\n",
    "        ),\n",
    "        # This transform is to check dice score per segment/label\n",
    "        SplitPredsLabeld(keys=\"pred\"),\n",
    "    ]\n",
    "    return Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720c2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(args, pre_transforms):\n",
    "    multi_gpu = args.multi_gpu\n",
    "    local_rank = args.local_rank\n",
    "\n",
    "    all_images = sorted(glob.glob(os.path.join(args.input, \"*.nii.gz\")))\n",
    "    all_labels = sorted(glob.glob(os.path.join(args.input, \"labels\", \"final\", \"*.nii.gz\")))\n",
    "    datalist = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in\n",
    "                zip(all_images, all_labels)]\n",
    "\n",
    "    datalist = datalist[0: args.limit] if args.limit else datalist\n",
    "    total_l = len(datalist)\n",
    "\n",
    "    if multi_gpu:\n",
    "        datalist = partition_dataset(\n",
    "            data=datalist,\n",
    "            num_partitions=dist.get_world_size(),\n",
    "            even_divisible=True,\n",
    "            shuffle=True,\n",
    "            seed=args.seed,\n",
    "        )[local_rank]\n",
    "\n",
    "    train_datalist, val_datalist = partition_dataset(\n",
    "        datalist,\n",
    "        ratios=[args.split, (1 - args.split)],\n",
    "        shuffle=True,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "    \n",
    "    logging.info(\n",
    "        \"CACHE_DIR IS: {}\".format(\n",
    "            args.cache_dir\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_ds = PersistentDataset(\n",
    "        train_datalist, pre_transforms, cache_dir=args.cache_dir\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, shuffle=True, num_workers=2\n",
    "    )\n",
    "    logging.info(\n",
    "        \"{}:: Total Records used for Training is: {}/{}\".format(\n",
    "            local_rank, len(train_ds), total_l\n",
    "        )\n",
    "    )\n",
    "\n",
    "    val_ds = PersistentDataset(val_datalist, pre_transforms, cache_dir=args.cache_dir)\n",
    "    val_loader = DataLoader(val_ds, num_workers=2)\n",
    "    logging.info(\n",
    "        \"{}:: Total Records used for Validation is: {}/{}\".format(\n",
    "            local_rank, len(val_ds), total_l\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7092b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trainer(args):\n",
    "\n",
    "    set_determinism(seed=args.seed)\n",
    "\n",
    "    multi_gpu = args.multi_gpu\n",
    "    local_rank = args.local_rank\n",
    "    if multi_gpu:\n",
    "        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "        device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device(\"cuda\" if args.use_gpu else \"cpu\")\n",
    "\n",
    "    pre_transforms = get_pre_transforms(args.labels, args.spatial_size)\n",
    "    click_transforms = get_click_transforms()\n",
    "    post_transform = get_post_transforms(args.labels)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(args, pre_transforms)\n",
    "\n",
    "    # define training components\n",
    "    network = get_network(args.network, args.labels, args.spatial_size).to(device)\n",
    "    if multi_gpu:\n",
    "        network = torch.nn.parallel.DistributedDataParallel(\n",
    "            network, device_ids=[local_rank], output_device=local_rank\n",
    "        )\n",
    "\n",
    "    if args.resume:\n",
    "        logging.info(\"{}:: Loading Network...\".format(local_rank))\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        network.load_state_dict(\n",
    "            torch.load(args.model_filepath, map_location=map_location)\n",
    "        )\n",
    "\n",
    "    # define event-handlers for engine\n",
    "    val_handlers = [\n",
    "        StatsHandler(output_transform=lambda x: None),\n",
    "        TensorBoardStatsHandler(log_dir=args.output, output_transform=lambda x: None),\n",
    "        CheckpointSaver(\n",
    "            save_dir=args.output,\n",
    "            save_dict={\"net\": network},\n",
    "            save_key_metric=True,\n",
    "            save_final=True,\n",
    "            save_interval=args.save_interval,\n",
    "            final_filename=\"pretrained_deepedit_\" + args.network + \".pt\",\n",
    "        ),\n",
    "    ]\n",
    "    val_handlers = val_handlers if local_rank == 0 else None\n",
    "\n",
    "    all_val_metrics = dict()\n",
    "    all_val_metrics[\"val_mean_dice\"] = MeanDice(\n",
    "        output_transform=from_engine([\"pred\", \"label\"]), include_background=False\n",
    "    )\n",
    "    for key_label in args.labels:\n",
    "        if key_label != \"background\":\n",
    "            all_val_metrics[key_label + \"_dice\"] = MeanDice(\n",
    "                output_transform=from_engine([\"pred_\" + key_label, \"label_\" + key_label]), include_background=False\n",
    "            )\n",
    "\n",
    "    evaluator = SupervisedEvaluator(\n",
    "        device=device,\n",
    "        val_data_loader=val_loader,\n",
    "        network=network,\n",
    "        iteration_update=Interaction(\n",
    "            deepgrow_probability=args.deepgrow_probability_val,\n",
    "            transforms=click_transforms,\n",
    "            click_probability_key=\"probability\",\n",
    "            train=False,\n",
    "            label_names=args.labels,\n",
    "        ),\n",
    "        inferer=SimpleInferer(),\n",
    "        postprocessing=post_transform,\n",
    "        key_val_metric=all_val_metrics,\n",
    "        val_handlers=val_handlers,\n",
    "    )\n",
    "\n",
    "    loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "    optimizer = torch.optim.Adam(network.parameters(), args.learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1)\n",
    "\n",
    "    train_handlers = [\n",
    "        LrScheduleHandler(lr_scheduler=lr_scheduler, print_lr=True),\n",
    "        ValidationHandler(\n",
    "            validator=evaluator, interval=args.val_freq, epoch_level=True\n",
    "        ),\n",
    "        StatsHandler(\n",
    "            tag_name=\"train_loss\", output_transform=from_engine([\"loss\"], first=True)\n",
    "        ),\n",
    "        TensorBoardStatsHandler(\n",
    "            log_dir=args.output,\n",
    "            tag_name=\"train_loss\",\n",
    "            output_transform=from_engine([\"loss\"], first=True),\n",
    "        ),\n",
    "        CheckpointSaver(\n",
    "            save_dir=args.output,\n",
    "            save_dict={\"net\": network, \"opt\": optimizer, \"lr\": lr_scheduler},\n",
    "            save_interval=args.save_interval * 2,\n",
    "            save_final=True,\n",
    "            final_filename=\"checkpoint.pt\",\n",
    "        ),\n",
    "    ]\n",
    "    train_handlers = train_handlers if local_rank == 0 else train_handlers[:2]\n",
    "\n",
    "    all_train_metrics = dict()\n",
    "    all_train_metrics[\"train_dice\"] = MeanDice(output_transform=from_engine([\"pred\", \"label\"]),\n",
    "                                               include_background=False)\n",
    "    for key_label in args.labels:\n",
    "        if key_label != \"background\":\n",
    "            all_train_metrics[key_label + \"_dice\"] = MeanDice(\n",
    "                output_transform=from_engine([\"pred_\" + key_label, \"label_\" + key_label]), include_background=False\n",
    "            )\n",
    "\n",
    "    trainer = SupervisedTrainer(\n",
    "        device=device,\n",
    "        max_epochs=args.epochs,\n",
    "        train_data_loader=train_loader,\n",
    "        network=network,\n",
    "        iteration_update=Interaction(\n",
    "            deepgrow_probability=args.deepgrow_probability_train,\n",
    "            transforms=click_transforms,\n",
    "            click_probability_key=\"probability\",\n",
    "            train=True,\n",
    "            label_names=args.labels,\n",
    "        ),\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_function,\n",
    "        inferer=SimpleInferer(),\n",
    "        postprocessing=post_transform,\n",
    "        amp=args.amp,\n",
    "        key_train_metric=all_train_metrics,\n",
    "        train_handlers=train_handlers,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6371066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    if args.local_rank == 0:\n",
    "        for arg in vars(args):\n",
    "            logging.info(\"USING:: {} = {}\".format(arg, getattr(args, arg)))\n",
    "        print(\"\")\n",
    "\n",
    "    if args.export:\n",
    "        logging.info(\n",
    "            \"{}:: Loading PT Model from: {}\".format(args.local_rank, args.input)\n",
    "        )\n",
    "        device = torch.device(\"cuda\" if args.use_gpu else \"cpu\")\n",
    "        network = get_network(args.network, args.labels, args.spatial_size).to(device)\n",
    "\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(args.local_rank)}\n",
    "        network.load_state_dict(torch.load(args.input, map_location=map_location))\n",
    "\n",
    "        logging.info(\"{}:: Saving TorchScript Model\".format(args.local_rank))\n",
    "        model_ts = torch.jit.script(network)\n",
    "        torch.jit.save(model_ts, os.path.join(args.output))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(args.output):\n",
    "        logging.info(\n",
    "            \"output path [{}] does not exist. creating it now.\".format(args.output)\n",
    "        )\n",
    "        os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "    trainer = create_trainer(args)\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.run()\n",
    "    end_time = time.time()\n",
    "\n",
    "    logging.info(\"Total Training Time {}\".format(end_time - start_time))\n",
    "    if args.local_rank == 0:\n",
    "        logging.info(\"{}:: Saving Final PT Model\".format(args.local_rank))\n",
    "        torch.save(\n",
    "            trainer.network.state_dict(), os.path.join(args.output, \"pretrained_deepedit_\" + args.network + \"-final.pt\")\n",
    "        )\n",
    "\n",
    "    if not args.multi_gpu:\n",
    "        logging.info(\"{}:: Saving TorchScript Model\".format(args.local_rank))\n",
    "        model_ts = torch.jit.script(trainer.network)\n",
    "        torch.jit.save(model_ts, os.path.join(args.output, \"pretrained_deepedit_\" + args.network + \"-final.ts\"))\n",
    "\n",
    "    if args.multi_gpu:\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf1f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strtobool(val):\n",
    "    return bool(distutils.util.strtobool(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a416ea-cbe5-41f0-8a0e-64894c63032e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optional Configuration Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aabcef-2bde-4217-a56f-eccfd847fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Default Configuration for Abdominal Segmentation Project\n",
    "\n",
    "#define paths for dataset input / model output, output will make subfolders according to dataset name\n",
    "global config\n",
    "config = {}\n",
    "config['dataset_path'] = \"/workspace/abdominal-segmentation/datasets\"\n",
    "config['output_path'] = \"/workspace/abdominal-segmentation/models\"\n",
    "#custom arg variables\n",
    "config['dataset'] = \"Task09_Spleen/imagesTr\"\n",
    "\n",
    "config['input'] = config['dataset_path'] + \"/\" + config['dataset']\n",
    "config['output'] = config['output_path'] + \"/\" + config['dataset']\n",
    "config['epochs'] = 100\n",
    "config['multi_gpu'] = False\n",
    "config['network'] = \"dynunet\"\n",
    "#config['num_samples'] = 4\n",
    "#defaults\n",
    "# config['use_gpu_bool'] = \"true\"\n",
    "# config['seednum'] = 36\n",
    "# config['amp_bool'] = \"false\"\n",
    "# config['split_float'] = 0.9\n",
    "# config['limit_int'] = 0\n",
    "# config['cache_dir_str'] = None\n",
    "# config['resume_bool'] = \"false\"\n",
    "# config['val_freq_int'] = 1\n",
    "# config['learning_rate_float'] = 0.0001\n",
    "# config['max_train_inter_int'] = 15\n",
    "# config['max_val_inter_int'] = 5\n",
    "# config['deepgrow_prob_train_float'] = 0.4\n",
    "# config['deepgrow_prob_val_float'] = 1.0\n",
    "# config['save_interval_int'] = 3\n",
    "# config['image_interval_int'] = 1\n",
    "# config['local_rank_int'] = 0\n",
    "# config['export_bool'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4329d61-ac37-4c9a-b3a2-ab841f743145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "os.chdir('/workspace/abdominal-segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe9240-853f-432e-851b-592a4a3e67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('configs/test.yaml', 'w') as outfile:\n",
    "#     yaml.dump(config, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd20844-ea23-4aa3-9e38-4c9130937d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['-f'] #+ ['-c'] + ['configs/test.yaml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6e418-6bbc-4b15-825a-49cb24ead3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_args(config,args=None):\n",
    "    #print(' Args: ', args)\n",
    "    if not args:\n",
    "        parser = argparse.ArgumentParser(description='Empty parser for config dict')\n",
    "        parser.add_argument('-dmy', '--dummy', action='store_true')\n",
    "        args = parser.parse_args()\n",
    "    for key,item in config.items():\n",
    "        args.__setattr__(key,item)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0630d5-1619-418e-9e5b-2c6b7412e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():  \n",
    "    parser = argparse.ArgumentParser(description='Configuration file to run deepedit Training')\n",
    "    args = None\n",
    "    try:\n",
    "        parser.add_argument(\"-s\", \"--seed\", type=int, default=36)\n",
    "        parser.add_argument(\"-n\", \"--network\", default=\"dynunet\", choices=[\"dynunet\", \"unetr\"])\n",
    "        parser.add_argument(\n",
    "            \"-i\",\n",
    "            \"--input\",\n",
    "            default=\"/home/andres/Documents/workspace/Datasets/MSD_datasets/Task09_Spleen\",\n",
    "        )\n",
    "        parser.add_argument(\"-o\", \"--output\", default=\"output\")\n",
    "\n",
    "        parser.add_argument(\"-g\", \"--use_gpu\", type=strtobool, default=\"true\")\n",
    "        parser.add_argument(\"-a\", \"--amp\", type=strtobool, default=\"false\")\n",
    "\n",
    "        parser.add_argument(\"-e\", \"--epochs\", type=int, default=100)\n",
    "        parser.add_argument(\"-x\", \"--split\", type=float, default=0.9)\n",
    "        parser.add_argument(\"-t\", \"--limit\", type=int, default=0)\n",
    "        parser.add_argument(\"--cache_dir\", type=str, default=None)\n",
    "\n",
    "        parser.add_argument(\"-r\", \"--resume\", type=strtobool, default=\"false\")\n",
    "\n",
    "        parser.add_argument(\"-f\", \"--val_freq\", type=int, default=1)\n",
    "        parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.0001)\n",
    "        parser.add_argument(\"-it\", \"--max_train_interactions\", type=int, default=15)\n",
    "        parser.add_argument(\"-iv\", \"--max_val_interactions\", type=int, default=5)\n",
    "\n",
    "        parser.add_argument(\"-dpt\", \"--deepgrow_probability_train\", type=float, default=0.4)\n",
    "        parser.add_argument(\"-dpv\", \"--deepgrow_probability_val\", type=float, default=1.0)\n",
    "\n",
    "        parser.add_argument(\"--save_interval\", type=int, default=3)\n",
    "        parser.add_argument(\"--image_interval\", type=int, default=1)\n",
    "        parser.add_argument(\"--multi_gpu\", type=strtobool, default=\"false\")\n",
    "        parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "        parser.add_argument(\"--export\", type=strtobool, default=\"false\")\n",
    "\n",
    "        parser.add_argument(\"-c\",'--config',type=str, default=None)\n",
    "        args = parser.parse_args()\n",
    "    except:\n",
    "        print('Cannot successfuly parse commandline arguments!')\n",
    "\n",
    "    if args:\n",
    "        if args.config:\n",
    "            print('Configuration file path found, reading settings from file', args.config)\n",
    "            with open(args.config) as conf_file:\n",
    "                configf = yaml.safe_load(conf_file)\n",
    "            args = dict_to_args(config, args)\n",
    "        else:\n",
    "            try:\n",
    "                if 'config' in globals():\n",
    "                    print('Using Configuration settings defined in source file')\n",
    "                    args = dict_to_args(config,args)\n",
    "            except Exception as e:\n",
    "                print('No default configuration settings defined in source file')\n",
    "                print(e)\n",
    "\n",
    "    if args:\n",
    "        args.spatial_size = [128, 128, 128]\n",
    "        # # For multiple label using the BTCV dataset (https://www.synapse.org/#!Synapse:syn3193805/wiki/217789)\n",
    "        # # For this, remember to update accordingly the function 'get_loaders' in lines 151-152\n",
    "        args.labels = {\"spleen\": 1,\"background\": 0,}\n",
    "#         args.labels = {\n",
    "#                    \"liver\": 1,\n",
    "#                 #   \"right kidney\": 2,\n",
    "#                 #   \"left kidney\": 3,\n",
    "#                 #   \"gallbladder\": 4,\n",
    "#                 #   \"esophagus\": 5,\n",
    "#                    \"spleen\": 2,\n",
    "#                 #   \"stomach\": 7,\n",
    "#                 #   \"aorta\": 8,\n",
    "#                    \"background\": 0,\n",
    "#                  }\n",
    "        # Restoring previous model if resume flag is True\n",
    "        #args.model_filepath = args.output + \"/net_key_metric=0.8566.pt\"\n",
    "        print('Training started with these settings: ', args)\n",
    "        run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16663e56-252d-49c6-be97-8d0dddc64a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a756a6-0785-40d0-98d2-ca2c8fdf56d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"[%(asctime)s.%(msecs)03d][%(levelname)5s](%(name)s) - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef8f6b-c05e-46a6-b111-7bf2b9f1531a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
