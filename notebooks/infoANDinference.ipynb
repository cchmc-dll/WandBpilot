{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DeepEdit Inference Tutorial - Edited by Elan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepEdit is an algorithm that combines the power of two models in one single architecture. It allows the user to perform inference, as a standard segmentation method (i.e. UNet), and also to interactively segment part of an image using clicks (Sakinis et al.). DeepEdit aims to facilitate the user experience and at the same time the development of new active learning techniques.\n",
    "\n",
    "\n",
    "This Notebooks shows the performance of a model trained to segment the spleen. \n",
    "\n",
    "**Once the model is trained, we recommend importing the pretrained model into the [DeepEdit App in MONAI Label](https://github.com/Project-MONAI/MONAILabel/tree/main/sample-apps/radiology#deepedit) for full experience.**\n",
    "\n",
    "Sakinis et al., Interactive segmentation of medical images through fully convolutional neural networks. (2019) https://arxiv.org/abs/1903.08205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"import monai\" || pip install -q \"monai-weekly[nibabel tqdm]\"\n",
    "# !python -c \"import matplotlib\" || pip install -q matplotlib==3.5.2\n",
    "# !pip install -q pytorch-ignite==0.4.8\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Library versions used:\n",
    "\n",
    "monai-weekly==0.9.dev2219 itk==5.2.1.post1 matplotlib==3.5.2 nibabel==3.2.2 numpy==1.22.3 pytorch-ignite==0.4.8 scikit-image==0.19.2 scipy==1.8.0 tensorboard==2.8.0 torch==1.11.0 tqdm==4.64.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import jit\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "\n",
    "from monai.apps.deepedit.transforms import (\n",
    "    AddGuidanceSignalDeepEditd,\n",
    "    AddGuidanceFromPointsDeepEditd,\n",
    "    ResizeGuidanceMultipleLabelDeepEditd,\n",
    ")\n",
    "\n",
    "\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Resized,\n",
    "    ScaleIntensityRanged,\n",
    "    SqueezeDimd,\n",
    "    ToNumpyd,\n",
    "    ToTensord,SaveImaged\n",
    ")\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(guidance, slice_idx):\n",
    "    if guidance is None:\n",
    "        return\n",
    "    for p in guidance:\n",
    "        p1 = p[1]\n",
    "        p2 = p[0]\n",
    "        plt.plot(p1, p2, \"r+\", 'MarkerSize', 30)\n",
    "\n",
    "\n",
    "def show_image(image, label, guidance=None, slice_idx=None):\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    if label is not None:\n",
    "        masked = np.ma.masked_where(label == 0, label)\n",
    "        plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "\n",
    "    draw_points(guidance, slice_idx)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if label is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"label\")\n",
    "        plt.imshow(label)\n",
    "        plt.colorbar()\n",
    "        # draw_points(guidance, slice_idx)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_data(data):\n",
    "    for k in data:\n",
    "        v = data[k]\n",
    "\n",
    "        d = type(v)\n",
    "        if type(v) in (int, float, bool, str, dict, tuple):\n",
    "            d = v\n",
    "        elif hasattr(v, 'shape'):\n",
    "            d = v.shape\n",
    "\n",
    "        if k in ('image_meta_dict', 'label_meta_dict'):\n",
    "            for m in data[k]:\n",
    "                print('{} Meta:: {} => {}'.format(k, m, data[k][m]))\n",
    "        else:\n",
    "            print('Data key: {} = {}'.format(k, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace/abdominal-segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Download data if not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and model\n",
    "\n",
    "# resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/_image.nii.gz\"\n",
    "# dst = \"_image.nii.gz\"\n",
    "\n",
    "# if not os.path.exists(dst):\n",
    "#     monai.apps.download_url(resource, dst)\n",
    "\n",
    "# resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/\\\n",
    "# download/0.8.1/pretrained_deepedit_dynunet-final.ts\"\n",
    "# dst = \"pretrained_deepedit_dynunet-final.ts\"\n",
    "# if not os.path.exists(dst):\n",
    "#     monai.apps.download_url(resource, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Define preprocessing transforms used during training to transform inference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = 'datasets/Task09_Spleen/imagesTs/spleen_1.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels = {'spleen': 1,\n",
    "          'background': 0\n",
    "          }\n",
    "\n",
    "# Pre Processing\n",
    "spatial_size = [128, 128, 128]\n",
    "\n",
    "input_image_path = 'datasets/Task09_Spleen/imagesTs/spleen_1.nii.gz'\n",
    "\n",
    "data = {\n",
    "    'image': input_image_path,\n",
    "    'guidance': {'spleen': [[66, 180, 105], [66, 180, 145]], 'background': []},\n",
    "}\n",
    "\n",
    "#slice_idx = original_slice_idx = data['guidance']['spleen'][0][2]\n",
    "slice_idx = 15\n",
    "\n",
    "pre_transforms = [\n",
    "    # Loading the image\n",
    "    LoadImaged(keys=\"image\", reader=\"ITKReader\"),\n",
    "    # Ensure channel first\n",
    "    EnsureChannelFirstd(keys=\"image\"),\n",
    "    # Change image orientation\n",
    "    Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
    "    # Scaling image intensity - works well for CT images\n",
    "    ScaleIntensityRanged(keys=\"image\", a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "    # DeepEdit Tranforms for Inference #\n",
    "    # Add guidance (points) in the form of tensors based on the user input\n",
    "    AddGuidanceFromPointsDeepEditd(ref_image=\"image\", guidance=\"guidance\", label_names=labels),\n",
    "    # Resize the image\n",
    "    Resized(keys=\"image\", spatial_size=spatial_size, mode=\"area\"),\n",
    "    # Resize the guidance based on the image resizing\n",
    "    ResizeGuidanceMultipleLabelDeepEditd(guidance=\"guidance\", ref_image=\"image\"),\n",
    "    # Add the guidance to the input image\n",
    "    AddGuidanceSignalDeepEditd(keys=\"image\", guidance=\"guidance\"),\n",
    "    # Convert image to tensor\n",
    "    ToTensord(keys=\"image\"),\n",
    "]\n",
    "\n",
    "original_image = None\n",
    "\n",
    "# Going through each of the pre_transforms\n",
    "original_shape = None\n",
    "\n",
    "for t in pre_transforms:\n",
    "    tname = type(t).__name__\n",
    "    data = t(data)\n",
    "    image = data['image']\n",
    "    label = data.get('label')\n",
    "    guidance = data.get('guidance')\n",
    "\n",
    "    print(\"{} => image shape: {}\".format(tname, image.shape))\n",
    "\n",
    "    if tname == 'LoadImaged':\n",
    "        original_image = data['image']\n",
    "        original_shape = original_image.shape\n",
    "        label = None\n",
    "        tmp_image = image[:, :, slice_idx]\n",
    "        show_image(tmp_image, label, [guidance['spleen'][0]], slice_idx)\n",
    "\n",
    "transformed_image = data['image']\n",
    "guidance = data.get('guidance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Load model from TS file, skip next section if using TS model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Using TS model\n",
    "# model_path = 'models/Task09_Spleen/imagesTr/pretrained_deepedit_dynunet-final.ts'\n",
    "# model = jit.load(model_path)\n",
    "# model.cuda()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Load model from PT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(network, labels, spatial_size):\n",
    "    # Network\n",
    "    if network == \"unetr\":\n",
    "        network = UNETR(\n",
    "            spatial_dims=3,\n",
    "            in_channels=len(labels) + 1,\n",
    "            out_channels=len(labels),\n",
    "            img_size=spatial_size,\n",
    "            feature_size=64,\n",
    "            hidden_size=1536,\n",
    "            mlp_dim=3072,\n",
    "            num_heads=48,\n",
    "            pos_embed=\"conv\",\n",
    "            norm_name=\"instance\",\n",
    "            res_block=True,\n",
    "        )\n",
    "    else:\n",
    "        network = DynUNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=len(labels) + 1,\n",
    "            out_channels=len(labels),\n",
    "            kernel_size=[3, 3, 3, 3, 3, 3],\n",
    "            strides=[1, 2, 2, 2, 2, [2, 2, 1]],\n",
    "            upsample_kernel_size=[2, 2, 2, 2, [2, 2, 1]],\n",
    "            norm_name=\"instance\",\n",
    "            deep_supervision=False,\n",
    "            res_block=True,\n",
    "        )\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PT model\n",
    "import distutils.util\n",
    "from monai.networks.nets import DynUNet, UNETR\n",
    "import torch.distributed as dist\n",
    "\n",
    "model_path = 'models/Task09_Spleen/imagesTr/pretrained_deepedit_dynunet-final.pt'\n",
    "#CheckPoint = torch.load(model_path)\n",
    "\n",
    "network = 'dynunet'\n",
    "labels = {\"spleen\": 1,\"background\": 0,}\n",
    "spatial_size = [128, 128, 128]\n",
    "use_gpu=True\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "network = get_network(network, labels, spatial_size).to(device)\n",
    "network.load_state_dict(torch.load(model_path))\n",
    "network.cuda()\n",
    "model = network.eval()\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "# optimizers.load_state_dict(CheckPoint['optimizer_state_dict'])\n",
    "# Epoch = CheckPoint['epoch']\n",
    "# Loss = CheckPoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Predict and display label and image overlay at model resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['image'][None].cuda()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "outputs = outputs[0]\n",
    "data['pred'] = outputs\n",
    "\n",
    "post_transforms = [\n",
    "    EnsureTyped(keys=\"pred\"),\n",
    "    Activationsd(keys=\"pred\", softmax=True),\n",
    "    AsDiscreted(keys=\"pred\", argmax=True),\n",
    "    SqueezeDimd(keys=\"pred\", dim=0),\n",
    "    ToNumpyd(keys=\"pred\"),\n",
    "]\n",
    "\n",
    "data_orig = data.copy()\n",
    "\n",
    "pred = None\n",
    "for t in post_transforms:\n",
    "    tname = type(t).__name__\n",
    "    data = t(data)\n",
    "    image = data['image']\n",
    "    label = data['pred']\n",
    "    print(\"{} => image shape: {}, pred shape: {}\".format(tname, image.shape, label.shape))\n",
    "\n",
    "for i in range(10, 110, 40):\n",
    "    image = transformed_image[0, :, :, i]  # Taking the first channel which is the main image\n",
    "    label = data['pred'][:, :, i]\n",
    "    if np.sum(label) == 0:\n",
    "        continue\n",
    "\n",
    "    print(\"Final PLOT:: {} => image shape: {}, pred shape: {}; min: {}, max: {}, sum: {}\".format(\n",
    "        i, image.shape, label.shape, np.min(label), np.max(label), np.sum(label)))\n",
    "    show_image(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Convert prediction back to original shape as scan image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting prediction image to original shape', original_shape)    \n",
    "post_transforms_resize =post_transforms[:-2] + [Resized(keys=[\"image\",\"pred\"], spatial_size=original_shape, mode=[\"area\",\"nearest\"])] + post_transforms[-2:]\n",
    "for t in post_transforms_resize:\n",
    "    tname = type(t).__name__\n",
    "    data_orig = t(data_orig)\n",
    "    image = data_orig['image']\n",
    "    label = data_orig['pred']\n",
    "    print(\"{} => image shape: {}, pred shape: {}\".format(tname, image.shape, label.shape))# print(\"Resized Final PLOT:: {} => image shape: {}, pred shape: {}; min: {}, max: {}, sum: {}\".format("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Images\n",
    "for i in range(5, original_shape[2], 5):\n",
    "    image = data_orig['image'][0, :, :, i]  # Taking the first channel which is the main image\n",
    "    label = data_orig['pred'][:, :, i]\n",
    "    if np.sum(label) == 0:\n",
    "        continue\n",
    "       \n",
    "    print(\"Final PLOT:: {} => image shape: {}, pred shape: {}; min: {}, max: {}, sum: {}\".format(\n",
    "        i, image.shape, label.shape, np.min(label), np.max(label), np.sum(label)))\n",
    "    show_image(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Saving using NiBabel - Does not work with Slicer3D, looks fine on ImageJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "output_dir = 'output/Task09_Spleen/imagesTs'\n",
    "ext = 'NIB.nii.gz'\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "original_affine = data_orig['image_meta_dict']['affine']\n",
    "print(original_affine)\n",
    "nib.save(nib.Nifti1Image(data_orig['pred'].astype(np.uint8), original_affine),\n",
    "     os.path.join(output_dir,'spleen_1_nib.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Saving using SaveImage Transform in MONAI - does not work with slicer3D, looks fine on ImageJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig['pred_meta_dict'] = data_orig['image_meta_dict']\n",
    "sd = SaveImaged(keys=[\"pred\"],output_dir=output_dir, output_postfix='SD', output_ext='.nii.gz',\n",
    "           squeeze_end_dims=True, \n",
    "           data_root_dir='datasets/Task09_Spleen/imagesTs',\n",
    "           writer=None)\n",
    "sd(data_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### Saving using SITK - Works with slicer3D for the Task09_Spleen dataset, needs path of original input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_3D_shape(nparray,option = 'channel_smallest'): # converts (H,W,C) to (C,H,W) arrays\n",
    "    print('Fixing 3D shape...')\n",
    "    if len(nparray.shape) == 3:\n",
    "        if option == 'channel_smallest': # channel is the smallest dimension\n",
    "            print('Input array shape: ', nparray.shape)\n",
    "            if np.argmin(nparray.shape) == 2:\n",
    "                nparray = np.moveaxis(nparray, 2, 0)\n",
    "        elif option == 'square_image':\n",
    "            if nparray.shape[0] == nparray.shape[1]:\n",
    "                nparray = np.moveaxis(nparray, 2, 0)\n",
    "        print('Output array shape: ', nparray.shape)\n",
    "        return nparray.astype('int16')\n",
    "    else:\n",
    "        #print('Not a 3D Array, shape: ')\n",
    "        raise Exception('Not a 3D array', nparray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "origImg = sitk.ReadImage(input_image_path)\n",
    "print('Orig Imag shape: ', origImg.GetSize())\n",
    "print('Label array shape: ', data_orig['pred'].astype(np.uint8).shape)\n",
    "sitkImg = sitk.GetImageFromArray(np.rot90(fix_3D_shape(data_orig['pred'].astype(np.uint8)), k=1, axes=(1,2)))\n",
    "sitkImg = sitk.Flip(sitkImg, [False, True, False])\n",
    "print('Pred Image shape: ',sitkImg.GetSize())\n",
    "sitkImg.CopyInformation(origImg)\n",
    "scan_out =os.path.join(output_dir,'spleen_1_sitk.nii.gz')\n",
    "sitk.WriteImage(sitkImg, scan_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
